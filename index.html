<!DOCTYPE html>
<html class=" w-mod-ix"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><style>.wf-force-outline-none[tabindex="-1"]:focus{outline:none;}</style>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YMTSZM1Z57"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YMTSZM1Z57');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Customizing Text-to-Image Diffusion with Camera Viewpoint Control</title>
    <link rel="stylesheet" href="images/bootstrap.min.css">
    <link href="images/css.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="images/Highlight-Clean.css">
    <link rel="stylesheet" href="images/styles.css">

    <link rel="manifest" href=images/site.webmanifest">

    <meta name="description" content="Customizing Text-to-Image Diffusion with Camera Viewpoint Control, 2023.">
    <meta property="og:url" content="https://customdiffusion360.github.io">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Customizing Text-to-Image Diffusion with Camera Viewpoint Control">
    <meta property="og:description" content="Customizing Text-to-Image Diffusion with Camera Viewpoint Control, 2023.">
    <meta property="og:image" content="https://customdiffusion360.github.io/images/mainpage/thumbnail.jpg">

    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="github.io">
    <meta property="twitter:url" content="https://customdiffusion360.github.io">
    <meta name="twitter:title" content="Customizing Text-to-Image Diffusion with Camera Viewpoint Control">
    <meta name="twitter:description" content="Customizing Text-to-Image Diffusion with Camera Viewpoint Control, 2023.">
    <meta name="twitter:image" content="https://customdiffusion360.github.io/images/mainpage/thumbnail.jpg">


    <script src="images/video_comparison.js"></script>
    <script type="module" src="images/model-viewer.min.js"></script>
    <style>
        strong {
          font-weight: bold;
        }
    </style>

  
</head>

<body>

    
    <div class="highlight-clean d-flex flex-column align-items-center justify-content-center" style="padding-bottom: 20px;">
        <div class="container" style="padding-bottom: 10px; max-width: 1000px;">
            <h1 class="text-center">Customizing Text-to-Image Diffusion with Camera Viewpoint Control</h1>
        </div>
        <div class="container  d-flex flex-column align-items-center justify-content-center" style="max-width: 900px;">
            <div class="row authors" style="padding-bottom: 20px;">
                <h5 class="text-center"><a class="text-center" href="https://nupurkmr9.github.io">Nupur Kumari</a><sup>1*</sup></h5>
                <h5 class="text-center"><a class="text-center" href=" https://graceduansu.github.io">Grace Su</a><sup>1*</sup></h5>
                <h5 class="text-center"><a href="https://richzhang.github.io">Richard Zhang</a><sup>2</sup></h5>
                <h5 class="text-center"><a href="https://taesung.me">Taesung Park</a><sup>2</sup></h5>
                <h5 class="text-center"><a class="text-center" href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><sup>2</sup></h5>
                <h5 class="text-center"><a class="text-center" href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><sup>1</sup></h5>
                
            </div>    
            <div class="row authors institute">
                <div class="col-sm-12">
                    <h3 class="text-center"><sup>1</sup> CMU  &nbsp&nbsp <sup>2</sup>Adobe Research</h3>
                </div>
            </div>    
        </div>
        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn btn-light" role="button" href="https://github.com/customdiffusion360/custom-diffusion360">
                <svg style="visibility:hidden;width:0px;height:24px;margin-left:-12px;margin-right:12px" width="0px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Code
            </a>
            <a class="btn btn-light" role="button" href="http://arxiv.org/abs/2404.12333">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>Paper
            </a>
            <a class="btn btn-light disabled border border-dark" aria-disabled="true" role="button" href="#">
                <svg style="visibility:hidden;width:0px;height:24px;margin-left:-12px;margin-right:12px" width="0px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Project
            </a>
            <a class="btn btn-light" role="button" href="results.html">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" width="24px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "/>
                </svg>
                Gallery
            </a>
             <a class="btn btn-light" role="button" href="https://huggingface.co/spaces/customdiffusion360/customdiffusion360">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 375 531">
                    <img src="images/huggingface_logo-noborder.svg" style="width:24px;height:24px;margin-left:-12px;margin-right:12px"/>
                </svg>
                Demo
            </a>
        </div>
    </div>

    <div class="container" style="max-width: 1200px;">
        <div class="row teaser">
            <div class="col-md-12">
                <!-- Large format devices -->
                <center><img src="images/mainpage/teaser.jpg" style="width: 90%;"/></center>
                <p>Our method allows additional control on the camera viewpoint of the custom object in generated images by text-to-image diffusion models, such as Stable Diffusion. Given a 360 degree mutliview dataset (~50 images), we fine-tune FeatureNeRF blocks in the intermediate feature space of the diffusion model to condition the generation of the object in a target camera pose.
                </p><p></p>
                <h6 class="caption"></h6>
            </div>
        </div>
    </div>


    <!-- <hr class="divider"> -->
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Abstract</h2></center>
                <!-- <h2>Abstract</h2> -->
                <p>
                    Model customization introduces new concepts to existing text-to-image models, enabling generation of the new concept in novel contexts.
                    However, such methods lack accurate camera view control w.r.t the object, and users must resort to prompt engineering (e.g., adding ``top-view'') to achieve coarse view control. In this work, we introduce a new task -- enabling explicit control of camera viewpoint for model customization. This allows us to modify object properties amongst various background scenes via text prompts, all while incorporating the target camera pose as additional control. This new task presents significant challenges in merging a 3D representation from the multi-view images of the new concept with a general, 2D text-to-image model. To bridge this gap, we propose to condition the 2D diffusion process on rendered, view-dependent features of the new object. During training, we jointly adapt the 2D diffusion modules and 3D feature predictions to reconstruct the object's appearance and geometry while reducing overfitting to the input multi-view images. Our method performs on par or better than existing image editing and model personalization baselines in preserving the custom object's identity while following the input text prompt and camera pose for the object.  
                </p>
                
            </div>
        </div>
    </div>

   

    <!-- <hr class="divider"> -->
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Algorithm</h2></center>
                <p>
                Given multi-view images of a target concept <strong>Teddy bear</strong> our method customizes a text-to-image diffusion model with that concept with an additional condition of target camera pose. We modify a subset of transformer layers to be pose-conditioned. This is done by adding a new FeatureNeRF block in intermediate feature space of the transformer layer. We finetune the new weights with the multiview dataset while keeping pre-trained model weights frozen. 
                </p>
                <p></p><p></p>
                <div style="text-align: center;">
                    <img src="images/mainpage/method.jpg" style="width: 80%;"/>
                  </div>
                <p></p><p></p>

            </div>
        </div>
    </div>

  
    <!-- <hr class="divider"> -->
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Comparison to Baselines</h2></center>
                All our results are based SDXL model. We customize the model on various categories of multiview images, e.g., car, teddybear, toys, motorcycle. We compare our method with image-editing methods, 3D NeRF editing methods, and Customization methods. Specifically, we comapre with <strong>SDEdit</strong>, <strong>LEDITS++</strong>, <strong>ViCA NeRF</strong>, and our proposed baseline of <strong>LoRA + Camera pose</strong> where we concatenate camera matrix with the text prompt.
                <p></p><p></p>
                <div style="text-align: center;">
                    <img src="images/mainpage/result.jpg" style="width: 80%;"/>
                  </div>

                <div id="singleconceptscroll" class="container" style="max-width: 1200px; ">

                <br>
                </div>
            </div>
        </div>
        <hr style="max-width: 1000px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Results</h2></center>
                We show results of varying the target camera pose and text prompt for each custom concept. More results are shown on the <a href="results.html">Gallery</a>.
                <p></p><p></p>
                <div id="singleconceptscroll" class="container" style="max-width: 1200px; ">
                <br>
                <div style="text-align: center;">
                    <img src="images/mainpage/results_vary_target_pose.jpg" style="width: 80%;"/>
                </div>
                
                </div>
            </div>
        </div>

        <hr style="max-width: 1000px;">

    </div>

    
    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Applications </h2></center>
                <p>  We show that our method can be used in various applications like generating the object in varying pose while keeping the same background using SDEdit or generating nice panorama scenes. We can also compose multiple instances of the object in FeatureNeRF space to generate scenes with desired relative camera pose between the two.
                </p> 
                <p></p><p></p>
                <div id="composition" class="container" style="max-width: 1200px;">
                    <center> <img  src="images/mainpage/application.jpg" style="width: 70%; margin-left: auto; margin-right: auto;" /> </center>
                        
                </div>
                <p></p><p></p>

            </div>
        </div>
    </div>

    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Limitations </h2></center>
                <p> Our method has still various limitations. It fails when we extrapolate the target camera pose far from the seen training views. The SDXL model has a bias to generate front facing object in such scenarios. It also sometimes fails to incoroporate the text prompt, especially when composing with another object.
                <p></p><p></p>
                <div id="limitation" class="container" style="max-width: 1200px;">

                    <center> <img  src="images/mainpage/limitation.jpg" style="width: 70%; margin-left: auto; margin-right: auto;" /> </center>
                        
                </div>
                <p></p><p></p>

            </div>
        </div>
    </div>



    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Citation</h2></center>
                <code>
                    @article{kumari2024customdiffusion360,<br>
                    &nbsp; author = {Kumari, Nupur and Su, Grace and Zhang, Richard and Park, Taesung and Shechtman, Eli and Zhu, Jun-Yan},<br>
                    &nbsp; title  = {Customizing Text-to-Image Diffusion with Camera Viewpoint Control},<br>
                    &nbsp; journal = {Arxiv},<br>
                    &nbsp; year   = {2024},<br>
                }</code>
            </div>
        </div>
    </div>

    <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Concurrent Works</h2></center>
                <h5>
                <ul>

                    <li> Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, and Niki Trigoni. <a href="https://ttchengab.github.io/continuous_3d_words">Learning Continuous 3D Words for Text-to-Image Generation.</a> CVPR 2024.</li><br>

                    <li> Lukas Höllein, Aljaž Božič, Norman Müller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhöfer, and Matthias Nießner. <a href="https://lukashoel.github.io/ViewDiff/">Viewdiff: 3d-consistent image generation with text-to-image models.</a> CVPR 2024.</li><br>

                </ul> 
            </h5>               
            </div>
        </div>
    </div>

    <!-- <hr style="max-width: 1200px;">
    <div class="container" style="max-width: 1200px;">
        <div class="row">
            <div class="col-md-12">
                <center><h2>Acknowledgements</h2></center>
                <p> We are thankful to Kangle Deng, Sheng-Yu Wang, and Gaurav Parmar for their helpful comments and discussion and to Sean Liu, Ruihan Gao, Yufei Ye, and Bharath Raj for proofreading the draft. This work was partly done by Nupur Kumari during the Adobe internship. The work is partly supported by Adobe Research, the Packard Fellowship, the Amazon Faculty Research Award, and NSF IIS-2239076. Grace Su is supported by the NSF Graduate Research Fellowship (Grant No. DGE2140739).
                </p>
            </div>
        </div>
    </div> -->






    <script src="images/polyfill.js"></script>
    <script src="images/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="images/scripts.js"></script>
    <script src="images/jquery.min.js"></script>
    <script src="images/bootstrap.bundle.min.js"></script>
    <script src="images/webflow.fd002feec.js"></script>
    
    <!-- Import the component -->



</body></html>